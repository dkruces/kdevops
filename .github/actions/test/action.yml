# SPDX-License-Identifier: GPL-2.0
---
name: Setup kdevops
description: Setup kdevops workspace

inputs:
  dir:
    description: 'Directory'
    required: true
    default: 'workdir'
  ci_workflow:
    required: false
    type: string
    default: 'demo'
  ci_mode:
    description: 'CI execution mode'
    required: false
    default: 'auto'
  force_validation:
    description: 'Force kdevops-ci validation mode (single test)'
    required: false
    default: 'false'
  force_full_testing:
    description: 'Force full testing mode (complete suite)'
    required: false
    default: 'false'

runs:
  using: "composite"
  steps:
    - name: Run CI tests
      working-directory: ${{ inputs.dir }}/kdevops
      shell: bash
      run: |
        set -euxo pipefail

        echo "=== DEBUG: Starting CI test execution ==="
        echo "CI_WORKFLOW: ${{ inputs.ci_workflow }}"
        echo "Working directory: $(pwd)"
        echo "=== DEBUG: Directory contents ==="
        ls -la

        # Create start time for duration calculation
        echo "$(date +%s)" > ci.start_time

        echo "=== DEBUG: Auto-detecting CI execution mode ==="
        echo "ci_mode input: '${{ inputs.ci_mode }}'"
        echo "force_validation input: '${{ inputs.force_validation }}'"
        echo "force_full_testing input: '${{ inputs.force_full_testing }}'"
        echo "github.event_name: '${{ github.event_name }}'"

        # Determine CI execution mode based on Option 2 strategy
        KDEVOPS_CI_MODE=""

        # Priority 1: GitHub context detection (PRs automatically get validation)
        if [[ "${{ github.event_name }}" == "pull_request"* ]]; then
          KDEVOPS_CI_MODE="validation"
          echo "Auto-detected: kdevops-ci validation (PR context)"

        # Priority 2: Force overrides from manual.yml
        elif [[ "${{ inputs.force_validation }}" == "true" ]]; then
          KDEVOPS_CI_MODE="validation"
          echo "User forced: kdevops-ci validation mode"

        elif [[ "${{ inputs.force_full_testing }}" == "true" ]]; then
          KDEVOPS_CI_MODE="full_testing"
          echo "User forced: full testing mode"

        # Priority 3: ci_mode choice from manual.yml
        elif [[ "${{ inputs.ci_mode }}" == "kdevops-validation" ]]; then
          KDEVOPS_CI_MODE="validation"
          echo "User selected: kdevops-ci validation mode"

        elif [[ "${{ inputs.ci_mode }}" == "full-testing" ]]; then
          KDEVOPS_CI_MODE="full_testing"
          echo "User selected: full testing mode"

        # Priority 4: Default based on trigger type
        elif [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
          KDEVOPS_CI_MODE="full_testing"
          echo "Auto-detected: full testing (manual dispatch default)"

        else
          # Fallback: validation mode for safety
          KDEVOPS_CI_MODE="validation"
          echo "Fallback: kdevops-ci validation mode"
        fi

        echo "Final CI mode: $KDEVOPS_CI_MODE"
        echo "KDEVOPS_CI_MODE=$KDEVOPS_CI_MODE" >> $GITHUB_ENV

        # Configure test execution based on detected/selected mode
        if [[ "$KDEVOPS_CI_MODE" == "validation" ]]; then
          echo "=== Configuring kdevops-ci validation mode (single test) ==="

          case "${{ inputs.ci_workflow }}" in
            *fstests*|*xfs*|*btrfs*|*ext4*|*tmpfs*)
              export TESTS="generic/003"
              echo "fstests validation: TESTS=$TESTS"
              ;;
            blktests*)
              export TESTS="block/003"
              echo "blktests validation: TESTS=$TESTS"
              ;;
            selftests*|*modules*|*kmod*|*firmware*|*mm*)
              export TESTS="kmod/test_001"
              echo "selftests validation: TESTS=$TESTS"
              ;;
            *)
              echo "Unknown workflow for validation, using minimal test"
              ;;
          esac

        else
          echo "=== Configuring full testing mode (complete suite) ==="
          echo "No TESTS parameter - running complete test suite"
          # Don't set TESTS variable - this triggers full suite
        fi

        make ci-test CI_WORKFLOW="${{ inputs.ci_workflow }}"

        echo "=== DEBUG: CI test completed, initializing metadata ==="
        echo -e "Kernel tests results:\n" > ci.commit_extra
        echo "=== DEBUG: Initial ci.commit_extra content ==="
        cat ci.commit_extra

    - name: Generate workflow results path
      id: setpath
      shell: bash
      run: |
        set -euxo pipefail

        echo "=== DEBUG: Determining workflow path ==="
        echo "ci_workflow: ${{ inputs.ci_workflow }}"

        case "${{ inputs.ci_workflow }}" in
          blktests*) wpath="workflows/blktests" ;;
          *btrfs*) wpath="workflows/fstests" ;;
          *ext4*) wpath="workflows/fstests" ;;
          tmpfs*) wpath="workflows/fstests" ;;
          *xfs*) wpath="workflows/fstests" ;;
          *) wpath="workflows/selftests" ;;
        esac

        echo "=== DEBUG: Resolved workflow path ==="
        echo "wpath: $wpath"

        echo "path=$wpath" >> $GITHUB_OUTPUT
        "${{ github.workspace }}/scripts/github_output.sh" wpath "$wpath"

    - name: Generate CI commit info with workflow-specific logic
      working-directory: ${{ inputs.dir }}/kdevops
      shell: bash
      run: |
        set -euxo pipefail

        echo "=== DEBUG: Starting result collection ==="
        echo "Working directory: $(pwd)"
        echo "=== DEBUG: Available files ==="
        ls -la

        wpath="${{ steps.setpath.outputs.wpath }}"
        echo "=== DEBUG: Workflow path resolved ==="
        echo "wpath: $wpath"

        # Workflow-specific result collection
        case "${{ inputs.ci_workflow }}" in
          *xfs*|*btrfs*|*ext4*|tmpfs*|*fstests*)
            # fstests workflows: Use xunit_results.txt for rich summary
            echo "Collecting fstests results..."
            if find "$wpath/results/last-run" -name "xunit_results.txt" -type f -exec cat {} \; > ci.commit_extra 2>/dev/null; then
              echo "Found xunit_results.txt"
            else
              echo "No xunit_results.txt found, using fallback..."
              echo "Kernel tests results:" > ci.commit_extra
              find "$wpath/results/last-run/" -name '*.dmesg.log' -exec tail -n 1 {} + >> ci.commit_extra 2>/dev/null || true
            fi

            # fstests success detection
            if ! grep -E "failures, [1-9]|errors, [1-9]" ci.commit_extra >/dev/null 2>&1; then
              echo "ok" > ci.result
            else
              echo "not ok" > ci.result
            fi
            ;;
          blktests*)
            set -euxo pipefail
            # blktests workflows: Collect individual test results
            echo "=== DEBUG: Starting blktests result collection ==="
            echo "Collecting blktests results..."
            echo "wpath: $wpath"
            if [ -d "$wpath/results/last-run" ]; then
              echo "Results directory exists: YES"
            else
              echo "Results directory exists: NO"
            fi

            if [ -d "$wpath/results/last-run" ]; then
              echo "=== DEBUG: Contents of results/last-run ==="
              ls -la "$wpath/results/last-run" || true
              echo "=== DEBUG: All files in results/last-run ==="
              find "$wpath/results/last-run" -type f | head -20 || true
            fi

            echo "Kernel tests results:" > ci.commit_extra

            # Collect test results from last-run directory
            if [ -d "$wpath/results/last-run" ]; then
              echo -e "\nBlktests summary:" >> ci.commit_extra

              # Count total tests, passed, and failed
              echo "=== DEBUG: Counting test files ==="
              all_files=$(find "$wpath/results/last-run" -name "*.out*" -o -name "*" -type f | grep -E '/[^/]*$')
              echo "All matching files:"
              echo "$all_files"
              total_tests=$(echo "$all_files" | wc -l)

              bad_files=$(find "$wpath/results/last-run" -name "*.out.bad" -type f)
              echo "Bad files found:"
              echo "$bad_files"
              failed_tests=$(echo "$bad_files" | grep -c . || echo 0)

              passed_tests=$((total_tests - failed_tests))

              echo "=== DEBUG: Test counts ==="
              echo "total_tests: $total_tests"
              echo "failed_tests: $failed_tests"
              echo "passed_tests: $passed_tests"

              echo "Tests run: $total_tests, Passed: $passed_tests, Failed: $failed_tests" >> ci.commit_extra

              # List failed tests if any
              if [ $failed_tests -gt 0 ]; then
                echo -e "\nFailed tests:" >> ci.commit_extra
                find "$wpath/results/last-run" -name "*.out.bad" -type f | sed 's|.*/\([^/]*\)\.out\.bad$|\1|' | sort >> ci.commit_extra 2>/dev/null || true
              fi

              # Show sample test status files for passed tests
              if [ $passed_tests -gt 0 ]; then
                echo -e "\nSample passed test:" >> ci.commit_extra
                echo "=== DEBUG: Looking for sample passed test ==="
                sample_files=$(find "$wpath/results/last-run" -type f -name "*" ! -name "*.out.bad" ! -name "*.dmesg")
                echo "Sample files found:"
                echo "$sample_files"
                sample_file=$(echo "$sample_files" | head -1)
                echo "Selected sample file: $sample_file"
                if [ -n "$sample_file" ] && [ -f "$sample_file" ]; then
                  echo "Sample file contents:"
                  cat "$sample_file" >> ci.commit_extra || echo "Failed to read sample file" >> ci.commit_extra
                else
                  echo "No valid sample file found" >> ci.commit_extra
                fi
              fi
            else
              echo -e "\nNo blktests results found in $wpath/results/last-run" >> ci.commit_extra
            fi

            # blktests success detection - look for .out.bad files (failures)
            echo "=== DEBUG: Determining test result ==="
            bad_check=$(find "$wpath/results/last-run" -name "*.out.bad" -type f | head -1)
            echo "Bad file check result: '$bad_check'"

            if [ -n "$bad_check" ]; then
              echo "=== DEBUG: Found bad files, setting result to 'not ok' ==="
              echo "not ok" > ci.result
            else
              echo "=== DEBUG: No bad files found, setting result to 'ok' ==="
              echo "ok" > ci.result
            fi

            echo "=== DEBUG: Final ci.result content ==="
            cat ci.result
            echo "=== DEBUG: Final ci.commit_extra content ==="
            cat ci.commit_extra
            ;;
          *selftests*|*modules*|*mm*|*firmware*)
            # selftests workflows: Use userspace.log
            echo "Collecting selftests results..."
            echo "Kernel tests results:" > ci.commit_extra
            find "$wpath/results/last-run/" -name '*.userspace.log' -exec cat {} \; >> ci.commit_extra 2>/dev/null || true

            # selftests success detection
            if grep -q "passed\|PASS" ci.commit_extra && ! grep -q "FAIL\|failed\|ERROR" ci.commit_extra; then
              echo "ok" > ci.result
            else
              echo "not ok" > ci.result
            fi
            ;;
          *)
            # Default/unknown workflows: Generic approach
            echo "Using generic result collection for ${{ inputs.ci_workflow }}..."
            echo "Kernel tests results:" > ci.commit_extra
            find "$wpath/results/last-run/" -name '*.dmesg.log' -exec tail -n 1 {} + >> ci.commit_extra 2>/dev/null || true
            echo -e "\n\nUserspace test results:" >> ci.commit_extra
            find "$wpath/results/last-run/" -name '*.userspace.log' -exec tail -n 1 {} + >> ci.commit_extra 2>/dev/null || true

            if grep -i -q "fail" ci.commit_extra; then
              echo "not ok" > ci.result
            else
              echo "ok" > ci.result
            fi
            ;;
        esac

        echo "=== DEBUG: Generating enhanced commit message ==="
        # Set environment variables for the commit message script
        export CI_WORKFLOW="${{ inputs.ci_workflow }}"
        export KERNEL_TREE="${{ inputs.kernel_tree || 'linux' }}"

        # Context-aware environment variables for auto-detection
        export KDEVOPS_CI_MODE="$KDEVOPS_CI_MODE"
        export GITHUB_EVENT_NAME="${{ github.event_name }}"

        # Only export TESTS if it's actually set and non-empty
        if [[ -n "${TESTS:-}" ]]; then
          export TESTS="$TESTS"
          echo "DEBUG: Exporting TESTS=$TESTS"
        else
          echo "DEBUG: TESTS not set - full testing mode"
        fi
        if [[ -n "${LIMIT_TESTS:-}" ]]; then
          export LIMIT_TESTS="$LIMIT_TESTS"
          echo "DEBUG: Exporting LIMIT_TESTS=$LIMIT_TESTS"
        fi

        # Generate enhanced commit message using our script
        ./scripts/generate_ci_commit_message.sh > ci.commit_message_enhanced

        echo "=== DEBUG: Enhanced commit message generated ==="
        echo "Enhanced commit message content:"
        cat ci.commit_message_enhanced

        # Keep the original ci.commit_extra for backward compatibility
        # The archive action will use ci.commit_message_enhanced if available
